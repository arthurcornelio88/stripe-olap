# ğŸ§  Stripe OLAP Pipeline

Transform raw Stripe dumps (OLTP) into clean OLAP data models with a fully automated ETL pipeline powered by Python, GCS, and Snowflake.

---

## ğŸ”„ Pipeline Overview

This project converts JSON dumps into analytics-ready Snowflake tables.

```

Raw JSON Dump
â†“
ETL Python (scripts/etl\_to\_snowflake.py)
â†“
fact\_ / dim\_ CSVs
â†“
Uploaded to GCS
â†“
Loaded into Snowflake (COPY INTO)
â†“
Views for BI (vw\_monthly\_revenue, vw\_customer\_ltv...)

````

ğŸ“„ For more context on the data modeling and transformations, see [`docs/fact_table.md`](docs/fact_table.md)

---

## ğŸ§¬ Environment Setup

Create the following environment files:

### `.env`

```ini
ENV=PROD
GCS_BUCKET=stripe-bucket-prod_v3             # anonymize if needed
GCP_CREDS_FILE=gcp_service_account.json      # anonymize if needed
````

### `.env.snowflake`

```ini
SNOWFLAKE_USER=STRIPEB2
SNOWFLAKE_PASSWORD=your_strong_password
SNOWFLAKE_ACCOUNT=UZPYBJI-XA86551
SNOWFLAKE_DATABASE=STRIPE_OLAP
SNOWFLAKE_SCHEMA=RAW
SNOWFLAKE_WAREHOUSE=WH_STRIPE_OLAP
```

> ğŸ” These should never be committed.

---

## ğŸ›  Usage via `make`

All commands are available via `make`. You can run any step independently or combine them.

### ğŸ§© Available commands

| Command                 | Description                                    |
| ----------------------- | ---------------------------------------------- |
| `make help`             | Show all available make targets                |
| `make oltp-olap`        | Run the JSON â†’ CSV transformation pipeline     |
| `make test`             | Run all tests                                  |
| `make test-offline`     | Run tests in GCS-offline/mock mode             |
| `make generate_create_tables`     | Creates sql create tables commands from OLTP .csv             |
| `make setup_snowflake`  | Create Snowflake infra (DB, schema, warehouse) |
| `make load_snowflake`   | Load data: create tables, views, COPY INTO     |
| `make dryrun_snowflake` | Preview all SQL commands without executing     |
| `make all`              | Run everything: transform + test + load        |

---

## ğŸ—‚ File Structure

```
.
â”œâ”€â”€ .env / .env.snowflake     # Secrets and config
â”œâ”€â”€ Makefile                  # Commands for the full pipeline
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ olap_io.py            # GCS CSV upload/download
â”‚   â”œâ”€â”€ etl_to_snowflake.py   # JSON â†’ CSV pipeline
â”‚   â”œâ”€â”€ load_to_snowflake.py  # Full Snowflake loader
â”‚   â”œâ”€â”€ gcp.py                # GCS credential config
â”‚   â””â”€â”€ sql/                  # All versioned SQL scripts
â”‚       â”œâ”€â”€ setup_snowflake_infra.sql
â”‚       â”œâ”€â”€ create_tables.sql
â”‚       â”œâ”€â”€ view_for_analytics.sql
â”‚       â”œâ”€â”€ copy_into_tables.sql
â”‚       â””â”€â”€ create_stage.sql
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ fact_table.md         # Explanation of OLTP vs OLAP tables
â”‚   â”œâ”€â”€ snowflake_etl.md      # Deep dive into Snowflake integration
â”‚   â””â”€â”€ snowflake_gcs.md      # How to configure GCS â†” Snowflake integration
â”œâ”€â”€ tests/                    # Pytest-based validation of tables
â””â”€â”€ olap_outputs/             # Local or GCS output location
```

---

## ğŸ” Full Pipeline Execution

```bash
make all ENV=PROD
```

This will:

1. Run the ETL
2. Upload the CSVs to GCS
3. Run validation tests
4. Load the latest OLAP dump into Snowflake

---

## â„ï¸ GCS â†” Snowflake Integration

To set up Snowflake external access to GCS via `STORAGE INTEGRATION`, see:

ğŸ‘‰ [`docs/snowflake_gcs.md`](docs/snowflake_gcs.md)

## ğŸš€ To Go Further

This project is fully operational â€” you can run it locally, validate it with tests, and inspect the result visually (see screenshots above).

To explore how the Snowflake schema was modeled:

ğŸ‘‰ [`docs/fact_dim.md`](docs/fact_dim.md)

To understand the full logic behind the GCS-to-Snowflake data loading process:

ğŸ‘‰ [`docs/snowflake_etl.md`](docs/snowflake_etl.md)

---